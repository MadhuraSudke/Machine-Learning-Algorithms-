# Simple Linear Regression From Scratch

This project implements **Simple Linear Regression from scratch using Python**, without using any machine learning libraries such as scikit-learn.

The goal is to understand:
- how linear regression works internally
- how gradient descent updates parameters
- why normalization is important
- the intuition behind fitting a straight line to data

---

## ğŸ“Œ Problem Statement

We use a **Salary Prediction dataset** where:
- **Input (X)**: Years of Experience  
- **Output (Y)**: Salary  

The objective is to learn a linear relationship:

Y = wX + b

where:
- `w` is the weight (slope)
- `b` is the bias (intercept)

---

## ğŸ“Š Dataset

The dataset contains two columns:
- `YearsExperience`
- `Salary`

This dataset shows a clear **positive linear relationship**, making it ideal for understanding simple linear regression.

---

## ğŸ§  Intuition Behind Linear Regression

Linear regression tries to find the **best-fit straight line** that minimizes the error between:
- actual values (Y)
- predicted values (Å·)

The model starts with random values for `w` and `b`, then gradually improves them using **Gradient Descent**.

---

## ğŸ“ Cost Function (Mean Squared Error)

We use the Mean Squared Error (MSE):

J(w, b) = (1 / 2n) Î£ (Å· âˆ’ y)Â²

This function measures how bad the predictions are.
Gradient descent minimizes this cost.

---

## ğŸ” Gradient Descent

Gradients tell us **how to change parameters** to reduce the loss.

âˆ‚J/âˆ‚w = (1 / n) Î£ (Å· âˆ’ y) x  
âˆ‚J/âˆ‚b = (1 / n) Î£ (Å· âˆ’ y)

Update rules:

w = w âˆ’ Î± âˆ‚J/âˆ‚w  
b = b âˆ’ Î± âˆ‚J/âˆ‚b  

where Î± is the learning rate.

---

## ğŸ“ Normalization (From Scratch)

To improve convergence, the input feature is normalized using **Z-score normalization**:

X_norm = (X âˆ’ Î¼) / Ïƒ

Normalization:
- centers data around 0
- improves gradient descent stability
- speeds up convergence

---

## ğŸ“ˆ Results

- The loss decreases smoothly over iterations
- The learned regression line fits the data well
- The slope correctly captures the positive relationship between experience and salary

---

## ğŸ›  Technologies Used

- Python
- NumPy
- Pandas
- Matplotlib

---

## ğŸš€ What This Project Demonstrates

âœ” Linear Regression from scratch  
âœ” Gradient Descent implementation  
âœ” Feature normalization  
âœ” Loss visualization  
âœ” Model intuition  

---

## ğŸ“Œ Future Improvements

- Train/Test split
- RÂ² score calculation
- Multivariate Linear Regression
- Optimizers like Momentum or Adam

---

## âœ¨ Author

Built for learning and understanding Machine Learning fundamentals.
